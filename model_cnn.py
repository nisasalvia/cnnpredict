# -*- coding: utf-8 -*-
"""SKRIPSI v.5.0

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GepRmjAWA8TLlMHtBMQZqTgVokzCewun

# 1. Import Dataset
"""

from google.colab import drive
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, f_classif
from scipy.stats import pearsonr
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics

# !pip install -q kaggle
import os

os.environ['KAGGLE_USERNAME'] = 'nisasalvia'
os.environ['KAGGLE_KEY'] = '75a45c7f7de251ea57c14063ec6fb0f9'

!kaggle datasets download -d uciml/pima-indians-diabetes-database --quiet
!unzip -q pima-indians-diabetes-database.zip

df = pd.read_csv('diabetes.csv')

df.head()

db = df.copy()
db

"""# 2. Preprocessing

## Cek Dataset
"""

len(df.columns)

pd.options.display.float_format = '{:.2f}'.format
df.describe()

df.info()

df.rename(columns={'DiabetesPedigreeFunction': 'DPF'}, inplace=True)
df

"""## Missing Values & Imputasi"""

missing_values = df.isnull().sum()
print("Missing values per column:\n", missing_values)

#mengecek nilai 0 pada dataset dan menggantinya ke NaN
df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.nan)
df.isnull().sum()

df.describe()

p = df.hist(figsize = (10,10))

#melakukan imputasi rata-rata berdasarkan outcome
imp_mean = ['Glucose', 'BloodPressure', 'SkinThickness', 'BMI']
df[imp_mean] = df.groupby('Outcome')[imp_mean].transform(lambda x: x.fillna(x.mean()))
df

#melakukan imputasi median berdasarkan outcome
imp_med = ['Insulin']
df[imp_med] = df.groupby('Outcome')[imp_med].transform(lambda x: x.fillna(x.median()))
df

p = df.hist(figsize = (10,10))

df.describe()

#cek data duplikat
duplicate_count = df.duplicated().sum()
print("Number of duplicate rows:", duplicate_count)

"""## Outlier"""

numerical_features = df.select_dtypes(include='number').columns
fig, ax = plt.subplots(1, len(numerical_features), figsize=(12, 10))

for i, feature in enumerate(numerical_features):
    ax[i].boxplot(df[feature])
    ax[i].set_title(feature)

plt.tight_layout()
plt.show()

"""notes :    
*   max skin thickness menurut jurnal dari analisis suku itu 45 mm, maybe bisa dibuat dari 50 mm
*   untuk bagian insulin paling normal adalah 222, mungkin bisa dibuat sampe 250 batasnya.


"""

df8i = df.copy() #IQR

"""### IQR"""

outlier = ['SkinThickness', 'Insulin', 'DPF']

def remove_outliers(dfis, columns, threshold=1.5):
  cleaned = []
  for df in dfis:
      df_clean = df.copy()
      for col in columns:
          Q1 = df_clean[col].quantile(0.25)
          Q3 = df_clean[col].quantile(0.75)
          IQR = Q3 - Q1
          lower_bound = Q1 - threshold * IQR
          upper_bound = Q3 + threshold * IQR
          df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]
      cleaned.append(df_clean)
  return cleaned

dfis = [df8i, df5i, df4i]

df8i, df5i, df4i = remove_outliers(dfis, outlier, threshold=2.0)  # Meningkatkan threshold dari 1.5 ke 2.0
df8i

df8i.describe()

numerical_features = df8i.select_dtypes(include='number').columns
fig, ax = plt.subplots(1, len(numerical_features), figsize=(10, 10))

for i, feature in enumerate(numerical_features):
    ax[i].boxplot(df8i[feature])
    ax[i].set_title(feature)

plt.tight_layout()
plt.show()

colors = ['#446BAD','#A2A2A2']

diabetes = df8i[df8i['Outcome'] == 1].describe().T
non_diabetes = df8i[df8i['Outcome'] == 0].describe().T

fig,ax = plt.subplots(nrows = 1,ncols = 2,figsize = (5,5))
plt.subplot(1,2,1)
sns.heatmap(diabetes[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f')
plt.title('Diabetes');

plt.subplot(1,2,2)
sns.heatmap(non_diabetes[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f',)
plt.title('Non-Diabetes');

fig.tight_layout(pad = -1)

"""## Normalisasi

MinMax digunakan untuk yang memiliki distribusi normal.
Standarisasi digunakan untuk yang memiliki skewed data.

### IQR
"""

df8i.hist(figsize = (10,10))

"""skewed : pregnancies, insulin, DPF, Age"""

from sklearn.preprocessing import MinMaxScaler,StandardScaler
mms = MinMaxScaler() # Normalization
ss = StandardScaler() # Standardization

standard = ['Pregnancies', 'Insulin', 'DPF', 'Age']
minmax = ['BloodPressure', 'SkinThickness', 'Glucose', 'BMI']

for df in [df8i, df5i, df4i]:
    for col in standard:
        df[col] = ss.fit_transform(df[[col]])
    for col in minmax:
        df[col] = mms.fit_transform(df[[col]])

df8i.head()

"""## Analisis Korelasi Pearson"""

plt.figure(figsize = (20,5))
sns.heatmap(df8i.corr(),cmap = 'viridis',annot = True);

fig, axs = plt.subplots(1, 3, figsize=(18, 6))

#df8i
corr_i = df8i.corrwith(df8i['Outcome']).sort_values(ascending=False).to_frame()
corr_i.columns = ['Correlations']
sns.heatmap(corr_i, annot=True, cmap='viridis', linewidths=0.4, linecolor='black', ax=axs[0])
axs[0].set_title('Correlation w.r.t Outcome (df8i)')

#df8k
corr_k = df8k.corrwith(df8k['Outcome']).sort_values(ascending=False).to_frame()
corr_k.columns = ['Correlations']
sns.heatmap(corr_k, annot=True, cmap='viridis', linewidths=0.4, linecolor='black', ax=axs[1])
axs[1].set_title('Correlation w.r.t Outcome (df8k)')

#df8w
corr_w = df8w.corrwith(df8w['Outcome']).sort_values(ascending=False).to_frame()
corr_w.columns = ['Correlations']
sns.heatmap(corr_w, annot=True, cmap='viridis', linewidths=0.4, linecolor='black', ax=axs[2])
axs[2].set_title('Correlation w.r.t Outcome (df8w)')

plt.tight_layout()
plt.show()

"""## Feature Engineering (ANNOVA)"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif

numerical_features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',
                      'Insulin', 'BMI', 'DPF', 'Age']
categorical_features = ['Outcome']

def get_anova_scores(df):
    features = df.loc[:, numerical_features]
    target = df.loc[:, categorical_features]
    selector = SelectKBest(score_func=f_classif, k='all')
    fit = selector.fit(features, target)
    scores_df = pd.DataFrame(fit.scores_, index=features.columns, columns=['ANOVA Score'])
    return scores_df.sort_values(by='ANOVA Score', ascending=False)

"""### 8 Fitur"""

anova_8i = get_anova_scores(df8i)
anova_8k = get_anova_scores(df8k)
anova_8w = get_anova_scores(df8w)

# Buat 3 heatmap berdampingan
fig, axs = plt.subplots(1, 3, figsize=(18, 6))

sns.heatmap(anova_8i, annot=True, cmap='viridis', linewidths=0.4, linecolor='black',
            fmt='.2f', ax=axs[0])
axs[0].set_title('ANOVA Scores (df8i)')

sns.heatmap(anova_8k, annot=True, cmap='viridis', linewidths=0.4, linecolor='black',
            fmt='.2f', ax=axs[1])
axs[1].set_title('ANOVA Scores (df8k)')

sns.heatmap(anova_8w, annot=True, cmap='viridis', linewidths=0.4, linecolor='black',
            fmt='.2f', ax=axs[2])
axs[2].set_title('ANOVA Scores (df8w)')

plt.tight_layout()
plt.show()

f8i = df8i.iloc[:,:8].values
t8i = df8i.iloc[:,8].values

f8k = df8k.iloc[:,:8].values
t8k = df8k.iloc[:,8].values

f8w = df8w.iloc[:,:8].values
t8w = df8w.iloc[:,8].values

"""## Sampling Data

### IQR
"""

import imblearn
from imblearn.over_sampling import SMOTE
from collections import Counter

smote = SMOTE(sampling_strategy='auto', random_state=42)
f8i, t8i = smote.fit_resample(f8i, t8i)
f5i, t5i = smote.fit_resample(f5i, t5i)
f4i, t4i = smote.fit_resample(f4i, t4i)

c8i = Counter(t8i)
c5i = Counter(t5i)
c4i = Counter(t4i)

counter_df = pd.DataFrame({
    't8i': pd.Series(c8i),
    't5i': pd.Series(c5i),
    't4i': pd.Series(c4i)
}).astype(int)

print(counter_df)

"""## Split Data"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_auc_score
from sklearn.metrics import RocCurveDisplay
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import precision_recall_curve

# 8 Feature Set
x_train_8i, x_test_8i, y_train_8i, y_test_8i = train_test_split(f8i, t8i, test_size=0.20, stratify=t8i, random_state=2)  # 8I
x_train_8k, x_test_8k, y_train_8k, y_test_8k = train_test_split(f8k, t8k, test_size=0.20, stratify=t8k, random_state=2)  # 8K
x_train_8w, x_test_8w, y_train_8w, y_test_8w = train_test_split(f8w, t8w, test_size=0.20, stratify=t8w, random_state=2)  # 8W

"""## Reshape data"""

# 8 Features
x_train_8i = x_train_8i.reshape(-1, 8, 1)
x_test_8i = x_test_8i.reshape(-1, 8, 1)

x_train_8k = x_train_8k.reshape(-1, 8, 1)
x_test_8k = x_test_8k.reshape(-1, 8, 1)

x_train_8w = x_train_8w.reshape(-1, 8, 1)
x_test_8w = x_test_8w.reshape(-1, 8, 1)

print(x_train_8i.shape)
print(x_test_8i.shape)

"""# Build Model CNN"""

from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam, AdamW
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2

param_learning_rates = [1e-2, 1e-3, 1e-4]
param_filters = [16, 32, 64]
param_kernel_size = [2, 3]

#untuk 4-5 fitur
param_kernel_size2 = [1, 2]
param_filters2 = [16, 32]

n_splits = 5
skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

"""## 8 Fitur"""

def build_model(
    input_shape=(8, 1),
    lr=0.001,          # learning rate
    n_filters=32,      # jumlah filter
    kernel_size=3,     # ukuran kernel
    dropout_rate=0.0,  # dropout
):
    model = Sequential()
    model.add(Conv1D(filters=n_filters,
                     kernel_size=kernel_size,
                     activation='relu',
                     input_shape=input_shape))
    model.add(MaxPooling1D(pool_size=2))

    # model.add(Conv1D(filters=n_filters*2, kernel_size=kernel_size, activation='relu'))
    # model.add(MaxPooling1D(pool_size=2))

    model.add(Flatten())

    if dropout_rate > 0:
        model.add(Dropout(dropout_rate))

    model.add(Dense(64, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))

    optimizer = Adam(learning_rate=lr)
    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    return model

"""### 8 IQR"""

best_config8i = None
best_accuracy8i = 0.0

# Looping semua kombinasi
for lr in param_learning_rates:
    for f in param_filters:
        for k in param_kernel_size:

            cv_accuracies8i = []

           # 4.1 Looping untuk tiap fold
            for train_index_8i, val_index_8i in skf.split(x_train_8i, y_train_8i):
                X_fold_train_8i = x_train_8i[train_index_8i]
                X_fold_val_8i   = x_train_8i[val_index_8i]
                y_fold_train_8i = y_train_8i[train_index_8i]
                y_fold_val_8i   = y_train_8i[val_index_8i]

                # 4.2 Build model
                model_8i = build_model(
                    input_shape=(8, 1),
                    lr=lr,
                    n_filters=f,
                    kernel_size=k,
                    dropout_rate=0.2
                )

                # Callback EarlyStopping
                es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

                # Training
                history8i = model_8i.fit(
                    x_train_8i, y_train_8i,
                    epochs=100,            #edit
                    batch_size=32,        #edit
                    validation_data=(X_fold_val_8i, y_fold_val_8i),
                    callbacks=[es],
                    verbose=0  # 1 jika ingin menampilkan progres training
                )

                # Evaluasi pada fold ini
                loss8i, accuracy8i = model_8i.evaluate(X_fold_val_8i, y_fold_val_8i, verbose=1)
                cv_accuracies8i.append(accuracy8i)

            # Rata-rata akurasi dari K-Fold
            mean_acc8i = np.mean(cv_accuracies8i)

            # Simpan config terbaik
            if mean_acc8i > best_accuracy8i:
                best_accuracy8i = mean_acc8i
                best_config8i = (lr, f, k)

            print(f"[INFO] LR={lr}, Filters={f}, KernelSize={k}, MeanCVAcc={mean_acc8i:.4f}")

print("========================================================")
print(f"Best Config: LR={best_config8i[0]}, Filters={best_config8i[1]}, Kernel={best_config8i[2]}")
print(f"Best Mean CV Accuracy: {best_accuracy8i:.4f}")
print("========================================================")

best_lr, best_filters, best_k = best_config8i

model_final_8i = build_model(
    input_shape=(8, 1),
    lr=best_lr,
    n_filters=best_filters,
    kernel_size=best_k,
    dropout_rate=0.2
)

es_final_8i = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Latih dengan semua data train (atau pakai validation_split, atau siapkan data val lain)
history_final_8i = model_final_8i.fit(
    x_train_8i, y_train_8i,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    callbacks=[es_final_8i],
    verbose=1
)

test_loss8i, test_acc8i = model_final_8i.evaluate(x_test_8i, y_test_8i, verbose=0)
print(f"Test Accuracy: {test_acc8i:.4f}")

import pickle
with open("summary_8i.pkl", "wb") as f:
    pickle.dump({
        "best_accuracy": best_accuracy8i,
        "test_accuracy": test_acc8i,
        "best_config":   best_config8i
    }, f)

"""### 8 KNN"""

best_config8k = None
best_accuracy8k = 0.0

# Looping semua kombinasi
for lr in param_learning_rates:
    for f in param_filters:
        for k in param_kernel_size:

            cv_accuracies8k = []

            # 4.1 Looping untuk tiap fold
            for train_index_8k, val_index_8k in skf.split(x_train_8k, y_train_8k):
                X_fold_train_8k = x_train_8k[train_index_8k]
                X_fold_val_8k   = x_train_8k[val_index_8k]
                y_fold_train_8k = y_train_8k[train_index_8k]
                y_fold_val_8k   = y_train_8k[val_index_8k]

                # 4.2 Build model
                model_8k = build_model(
                    input_shape=(8, 1),
                    lr=lr,
                    n_filters=f,
                    kernel_size=k,
                    dropout_rate=0.2
                )

                # Callback EarlyStopping
                es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

                # Training
                history8k = model_8k.fit(
                    X_fold_train_8k, y_fold_train_8k,
                    epochs=100,            # bisa disesuaikan
                    batch_size=32,        # bisa disesuaikan
                    validation_data=(X_fold_val_8k, y_fold_val_8k),
                    callbacks=[es],
                    verbose=0  # ubah ke 1 jika ingin lihat progres training
                )

                # Evaluasi pada fold ini
                loss8k, accuracy8k = model_8k.evaluate(X_fold_val_8k, y_fold_val_8k, verbose=1)
                cv_accuracies8k.append(accuracy8k)

            # Rata-rata akurasi dari K-Fold
            mean_acc8k = np.mean(cv_accuracies8k)

            # Simpan config terbaik
            if mean_acc8k > best_accuracy8k:
                best_accuracy8k = mean_acc8k
                best_config8k = (lr, f, k)

            print(f"[INFO] LR={lr}, Filters={f}, KernelSize={k}, MeanCVAcc={mean_acc8k:.4f}")

print("========================================================")
print(f"Best Config: LR={best_config8k[0]}, Filters={best_config8k[1]}, Kernel={best_config8k[2]}")
print(f"Best Mean CV Accuracy: {best_accuracy8k:.4f}")
print("========================================================")

best_lr, best_filters, best_k = best_config8k

model_final_8k = build_model(
    input_shape=(8, 1),
    lr=best_lr,
    n_filters=best_filters,
    kernel_size=best_k,
    dropout_rate=0.2)

es_final_8k = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Latih dengan semua data train (atau pakai validation_split, atau siapkan data val lain)
history_final_8k = model_final_8k.fit(
    x_train_8k, y_train_8k,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    callbacks=[es_final_8k],
    verbose=1)

test_loss8k, test_acc8k = model_final_8k.evaluate(x_test_8k, y_test_8k, verbose=0)
print(f"Test Accuracy: {test_acc8k:.4f}")

with open("summary_8k.pkl", "wb") as f:
    pickle.dump({
        "best_accuracy": best_accuracy8k,
        "test_accuracy": test_acc8k,
        "best_config":   best_config8k
    }, f)

"""### 8 Winsoring"""

best_config8w = None
best_accuracy8w = 0.0

# Looping semua kombinasi
for lr in param_learning_rates:
    for f in param_filters:
        for k in param_kernel_size:

            cv_accuracies8w = []

            # 4.1 Looping untuk tiap fold
            for train_index_8w, val_index_8w in skf.split(x_train_8w, y_train_8w):
                X_fold_train_8w = x_train_8w[train_index_8w]
                X_fold_val_8w   = x_train_8w[val_index_8w]
                y_fold_train_8w = y_train_8w[train_index_8w]
                y_fold_val_8w   = y_train_8w[val_index_8w]

                # 4.2 Build model
                model_8w = build_model(
                    input_shape=(8, 1),
                    lr=lr,
                    n_filters=f,
                    kernel_size=k,
                    dropout_rate=0.2
                )

                # Callback EarlyStopping
                es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

                # Training
                history8w = model_8w.fit(
                    X_fold_train_8w, y_fold_train_8w,
                    epochs=100,            # bisa disesuaikan
                    batch_size=32,        # bisa disesuaikan
                    validation_data=(X_fold_val_8w, y_fold_val_8w),
                    callbacks=[es],
                    verbose=0  # ubah ke 1 jika ingin lihat progres training
                )

                # Evaluasi pada fold ini
                loss8w, accuracy8w = model_8w.evaluate(X_fold_val_8w, y_fold_val_8w, verbose=1)
                cv_accuracies8w.append(accuracy8w)

            # Rata-rata akurasi dari K-Fold
            mean_acc8w = np.mean(cv_accuracies8w)

            # Simpan config terbaik
            if mean_acc8w > best_accuracy8w:
                best_accuracy8w = mean_acc8w
                best_config8w = (lr, f, k)

            print(f"[INFO] LR={lr}, Filters={f}, KernelSize={k}, MeanCVAcc={mean_acc8w:.4f}")

print("========================================================")
print(f"Best Config: LR={best_config8w[0]}, Filters={best_config8w[1]}, Kernel={best_config8w[2]}")
print(f"Best Mean CV Accuracy: {best_accuracy8w:.4f}")
print("========================================================")

best_lr, best_filters, best_k = best_config8w

model_final_8w = build_model(
    input_shape=(8, 1),
    lr=best_lr,
    n_filters=best_filters,
    kernel_size=best_k,
    dropout_rate=0.2)

es_final_8w = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Latih dengan semua data train (atau pakai validation_split, atau siapkan data val lain)
history_final_8w = model_final_8w.fit(
    x_train_8w, y_train_8w,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    callbacks=[es_final_8w],
    verbose=1)

test_loss8w, test_acc8w = model_final_8w.evaluate(x_test_8w, y_test_8w, verbose=0)
print(f"Test Accuracy: {test_acc8w:.4f}")

with open("summary_8w.pkl", "wb") as f:
    pickle.dump({
        "best_accuracy": best_accuracy8w,
        "test_accuracy": test_acc8w,
        "best_config":   best_config8w
    }, f)

"""### Result 8 Fitur"""

plt.figure(figsize=(18, 5))

plt.subplot(1, 3, 1)
plt.plot(history8i.history['loss'], label='Train')
plt.plot(history8i.history['val_loss'], label='Val')
plt.title('Loss - 8i (IQR)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(history8k.history['loss'], label='Train')
plt.plot(history8k.history['val_loss'], label='Val')
plt.title('Loss - 8k (KNN)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 3, 3)
plt.plot(history8w.history['loss'], label='Train')
plt.plot(history8w.history['val_loss'], label='Val')
plt.title('Loss - 8w (Winsor)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

plt.figure(figsize=(18, 5))

plt.subplot(1, 3, 1)
plt.plot(history8i.history['accuracy'], label='Train')
plt.plot(history8i.history['val_accuracy'], label='Val')
plt.title('Accuracy - 8i (IQR)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(history8k.history['accuracy'], label='Train')
plt.plot(history8k.history['val_accuracy'], label='Val')
plt.title('Accuracy - 8k (KNN)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 3, 3)
plt.plot(history8w.history['accuracy'], label='Train')
plt.plot(history8w.history['val_accuracy'], label='Val')
plt.title('Accuracy - 8w (Winsor)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

with open("summary_8i.pkl", "rb") as f: s8i = pickle.load(f)
with open("summary_8k.pkl", "rb") as f: s8k = pickle.load(f)
with open("summary_8w.pkl", "rb") as f: s8w = pickle.load(f)

results_8 = pd.DataFrame({
    "Metode Outlier": ["IQR", "KNN", "Winsorizing"],
    "Train Accuracy": [s8i["best_accuracy"], s8k["best_accuracy"], s8w["best_accuracy"]],
    "Test Accuracy":  [s8i["test_accuracy"], s8k["test_accuracy"], s8w["test_accuracy"]],
    "Best LR":        [s8i["best_config"][0], s8k["best_config"][0], s8w["best_config"][0]],
    "Best Filters":   [s8i["best_config"][1], s8k["best_config"][1], s8w["best_config"][1]],
    "Best Kernel":    [s8i["best_config"][2], s8k["best_config"][2], s8w["best_config"][2]],
})
results_8

print("\n--- Final 8 Feature model architecture ---")
model_final_8i.summary()